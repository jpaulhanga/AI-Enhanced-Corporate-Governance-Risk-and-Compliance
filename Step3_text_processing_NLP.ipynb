{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0sMtRPWm2SxU2neBir9Z1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Step 3: NLP Feature Extraction\n","import pandas as pd\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","# Load cleaned text\n","text_df = pd.read_csv(\"unified_cleaned_text.csv\")  # ticker, year, clean_text\n","\n","# Initialize FinBERT (for financial 10-K text)\n","tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n","model = AutoModel.from_pretrained(\"yiyanghkust/finbert-tone\")\n","\n","# Function to convert text to embedding\n","def embed_text(text):\n","    tokens = tokenizer(str(text), return_tensors='pt', truncation=True, padding=True, max_length=512)\n","    with torch.no_grad():\n","        outputs = model(**tokens)\n","    embedding = outputs.last_hidden_state.mean(dim=1).numpy()  # mean pooling\n","    return embedding[0]\n","\n","# Generate embeddings\n","embeddings = []\n","for i, txt in enumerate(text_df['clean_text']):\n","    embeddings.append(embed_text(txt))\n","    if (i+1) % 100 == 0:\n","        print(f\"Processed {i+1}/{len(text_df)} texts\")\n","\n","X_text = np.array(embeddings)\n","np.save(\"X_text.npy\", X_text)\n","print(\"✅ Saved FinBERT embeddings → X_text.npy, shape:\", X_text.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4EhCOx590HiQ","executionInfo":{"status":"ok","timestamp":1761537472041,"user_tz":300,"elapsed":13176,"user":{"displayName":"Bharath Kumar Uppala","userId":"01635682214238767727"}},"outputId":"d2ca90d9-f2e8-4dd3-ebd8-14a771d28fdb"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Saved FinBERT embeddings → X_text.npy, shape: (4, 768)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"8rz4kmru0IJJ"},"execution_count":null,"outputs":[]}]}